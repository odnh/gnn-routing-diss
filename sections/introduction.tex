\chapter{Introduction}

% this has to be done after the chapter start so can't put in top level :'(
\pagenumbering{arabic}
\setcounter{page}{1}

Routing has always been an integral part of the functioning of the internet as it is required for data to successfully traverse a network from source to destination. Historically, strategies have mainly focussed on those that can be calculated in a distributed manner, are functionally correct, and are robust to network changes all while providing reasonable performance (both in the time to calculate routes and their effect on the traffic itself). More recently, and especially with the advent of software defined networking (SDN) there has been a concerted effort to exert more control over how traffic is routed. This has either been used to implement policies that favour types of traffic, particular customers or simply aims to achieve particular notions of performance on the network.

In the area of intradomain routing where this kind of control is possible, one particular focus has been on minimising link over-utilisation as congestion can have large impacts on network performance for the end-users. To perform this successfully, such protocols must take into account one traffic flow on the network impacts the other traffic flows which leads us to data-driven routing. It is already know how to route optimally given a set of known traffic demands. However, these in general cannot be known in advance. Therefore, there have been efforts to create routing schemes that will generally give good congestion performance no matter the particular traffic demands. Further research has aimed to make this include some notion of current traffic.

A recent paper: ``Learning To Route with Deep RL''\cite{valadarsky2017learning} examined whether, under the assumption that there is some information in the history of traffic demands on a network relevant to future demands on that same network, we can use reinforcement learning to perform routing for the current (unknown) network conditions that is closer to the optimal than any scheme that can be created in advance without taking into account such information. This work produced some impressive result. However, once it has learnt to route on a particular network, due to the rigid structure of the neural network used to make the policy this cannot be applied to a different network. This issue may seem fairly small, that is until you take into account the fact that networks often change, especially due to temporary outages. If, for example, a link or node in the network were to be temporarily off-line then such a system would be unable to provide a routing scheme.

This project seeks to take the problem specified in ``Learning to Route with Deep RL'' as well as its solution and extend its domain to cover changing the structure of network itself. In other words, the aim is to use reinforcement learning (RL) to be able to create close-to-optimal routings for a network given a history of traffic demands on that network and that this should be able to generalise both over different demand sequences for the same network and different demand sequences for entirely different networks. Finally, we aim to show that this is not just a toy problem and solution but in fact leads to results on real-world datasets.

In summary, this study makes the following research contributions:
\begin{itemize}
  \item Provides an environment for experimenting with RL in data-driven routing
  \item Designs a new mapping from edge weights to a fully specified routing
  \item Introduces policy designs for approaching data-driven routing in a way that is generalisable
  \item Presents a comprehensive assessment of the performance of different techniques with a specific focus on generalisability over traffic patterns and network topologies.
\end{itemize}

The rest of the dissertation is structured as follows: chapter~\ref{chapter:background} introduces both previous research in this area and the research on which techniques in this work are based; chapter~\ref{chapter:problem} presents the formal specification of the problem, how the environment was designed, and techniques that made the problem and routing feasible to be performed using RL; chapter~\ref{chapter:learning} describes how the RL policy was designed and trained, and the structure of the GNNs used; chapter~\ref{chapter:evaluation} explains the evaluation framework and examines the results of the experiments performed; and chapter~\ref{chapter:conclusions} summaries the findings and contributions of the entire work as well as providing an insight into possible future work.
