\chapter{Routing agent policy design}
\label{chapter:learning}

 \section{Introduction}
In this chapter, we describe the basic policies used in previous work and their shortcomings, the policies that we have implemented, and their benefits. We also discuss how these policies were designed, and the structure of learning used to achieve the best results.

\section{Environment interface}
We describe he general interface between agent and environment for this problem in chapter~\ref{chapter:problem}. However, it omitted one important detail. When using the softmin routing strategy, one of the parameters for input to the softmin function was a value $\gamma$. In their paper, Valadarsky et al.\cite{valadarsky2017learning} set this value to $2.0$. They also only ran their experiments on a single graph. We quickly realised that the best value to use for $\gamma$ would also depend on the topology of the graph used, and so although it was a hyperparameter in prior work, now that the graph changing is part of the problem space, $\gamma$ also has to be output from our agent and so it is placed into the action space.

\section{Standard policies}
We use two policies as baselines against which we compare the new \ac{gnn}-based method. The first of these is an \acf{mlp}\cite{rumelhart1986learning}. In this case, we use a fully-connected network as which is the type used in previous work. It is important to note that this policy does not take into account any of the structure of the problem, so it is, in effect, the simplest approach. However, it is also the least flexible as its input and output sizes are both fixed. This lack of flexibility means that it can only be trained and used on one graph and with one length of history as the observations must fit the input.

We then extend this model to use an \acf{lstm}\cite{hochreiter1997long} as the history structure of the observations is limiting and only works if we know it is longer than any regularity we wish to find. The \ac{lstm} model is different in that at each timestep the observation it receives is only the previous \ac{dm} with the hope that it will learn to interpret the history in some meaningful way using its recurrent memory. This is beneficial as it allows for flexibility of history length and means that less structure in the problem is assumed. However, it still suffers from the same issue as the \ac{mlp}. Namely, it has a fixed size input and output, meaning that it can only be trained and used on a single graph. In fact, it is potentially possible to use an \ac{lstm} in a similar way to the iterative \ac{gnn} model described in section~\ref{section:gnn_iterative}, but that is beyond the scope of this research. A diagram of the \ac{mlp} and \ac{lstm} policies can be seen in figure~\ref{fig:mlp_lstm}.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/mlp_lstm.pdf}
    \caption{The structure of the \ac{mlp} and \ac{lstm} policies for data-driven routing. On the left is a representation of the \ac{mlp} policy and on the right is a representation of the \ac{lstm} policy.}
    \label{fig:mlp_lstm}
\end{figure}


\section{\acs{gnn} policy}
\label{section:gnn_policy}
The new work is introduced here with the application of \acp{gnn} to the problem. The first stage of this was to make sure that a \ac{gnn} can perform routing to the same level as the \ac{mlp}. To achieve this, we began with the constrained space where we read out an entire routing from the \ac{gnn} policy as a single action. Due to the nature of training frameworks, this, unfortunately, allows us only to train the network on one graph as the output size must be fixed and is the number of edges of the graph. However, it does still allow us to apply the trained model to different graphs.

When deciding what form the \ac{gnn} should take, there are many different types from which to choose. For example, there is the \ac{gat}\cite{velivckovic2017graph} which uses self-attention when propagating and is parallelisable, and \acp{mpnn}\cite{gilmer2017neural}, which first generalised different types of \ac{gnn} that pass data along edges and combine it in some way. In this specific problem, we need outputs on edges and inputs on nodes, so the model most derive edge features taking into account node features. It is also the case that the amount of demand to and from a specific node can have a global impact, as such a flow may be destined for the most distant vertex from this one. Therefore, the structure will have to be able to take into account global information when setting edge weights. Finally, the structure of the graph in the way its edges connect has a significant impact on how flows can be routed and their resulting impact on utilisation. To properly take advantage of this, our policy must be able to see how information flows and so must make use of the edges. Given these requirements, it makes sense to use one of the most general kinds of \ac{gnn} so as not to place accidental restrictions on learning.

In keeping with the above observations, we settled on using a fully connected graph network block as defined by Battaglia et al.\cite{battaglia2018relational} and shown in section~\ref{section:graph_neural_networks} as this is the most general method achievable with the framework and means that we have not constricted any information flows unnecessarily. We are also aware that constraining the feature vectors for edges and vertices to length one and two respectively, due to the input and output sizes, may make learning less efficient and results worse. Therefore, we expand from the core processing \ac{gn} block to an \enquote{encode-process-decode} model. This model consists of first mapping all the node features using a learned function to larger hidden size, then performing some number of computation steps by the core network and finally mapping all of the edge features to the correct output size using another learned function. The structure of this policy can be seen in figure~\ref{fig:encode_process_decode}.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/encode_process_decode.pdf}
    \caption{The encode-process-decode \ac{gnn} structure. On the left are the inputs to the policy, followed by the attribute encoding block. Then we have the full graph network block which performs the message passing and the core of the computation. Finally on the right is the attribute decoding block and the policy outputs. We can see an extra loop from output to input on the central block, which is used in the case that we have multiple message-passing steps to carry back the updated state.}
    \label{fig:encode_process_decode}
\end{figure}

Inside the graph network blocks, we use \acp{mlp} with learned parameters for all the attribute update $\phi$ functions (separate \acp{mlp} for updating the edge, vertex, and global attributes). For the pooling $\rho$ functions, we use the \texttt{tf.unsorted\_segment\_sum} function to aggregate attributes across vertices and edges.

In summary, the inputs are ingoing and outgoing demands for each vertex:
\begin{equation}
  \label{equation:node_inputs}
  \bm{V} = \left\{\left(\sum_{j=1}^{|V|}{D_{ij}}, \sum_{j=1}^{|V|}{D_{ji}}\right)\right\}_{i=1:|V|}
\end{equation}

and the outputs are a weight for each edge:
\begin{equation}
  \label{equation:edge_outputs}
  \bm{E} = \{w\}_{i=1:|E|}
\end{equation}


\section{Iterative \acs{gnn} policy}
\label{section:gnn_iterative}
Given the caveats of the basic \ac{gnn} policy described above, it is necessary to design a policy that can be trained on different graphs as well as run on different graphs which hopefully leads to better generalisation. For this to work, we need a fixed-size action space. Fortunately, similar problems have been encountered before, so we were able to take inspiration from works such as Placeto\cite{venkatakrishnan2019learning} to create an iterative \ac{rl} approach to routing.

In the iterative approach, we only output the value for a single edge as each action, meaning that the network has to be able to output the value for the right edge in the correct action step. The way that this is implemented is by providing extra information in the observation to help with the embedding. In Placeto, Ravichandra et al.\ use an \acf{rnn} with a hand-designed method of pooling and message-passing steps to build a graph embedding feature vector that can be passed as input to the \ac{rnn} in each step. We instead harness the more recent research on \acp{gnn} letting a \ac{gnn} learn the embedding itself. To do this, we modified the simple \ac{gnn} policy introduced above.

Getting a set of edge weights for a routing strategy from the policy given a demand history takes place in the course of a set of iterations. This is because we can only set one edge value at a time and there are (generally) multiple edges. To enable this, we encode the information of which edge we wish to set on the inputs. As before, the node attribute inputs are the demand history, as shown in equation~\ref{equation:node_inputs}---however, this time we also create input attributes for the edges. The input attribute vector per edge becomes the 3-tuple defined in equation~\ref{equation:edge_inputs}. That is, a value in the interval $[-1,1]$ denoting the current value for this edge in the routing (and 0 if not set), a binary value from the set $\{0,1\}$ denoting whether this edge's value has already been set in the routing, and a binary value from the set $\{0,1\}$ denoting whether this is the edge to be set in the current iteration or not.

\begin{equation}
  \label{equation:edge_inputs}
  E = \left\{ (\operatorname{weight}_i, \operatorname{set\vphantom{_i}}_i, \operatorname{target}_i) \right\}_{i=1:|E|}
\end{equation}

These observation inputs are passed into the same encode-process-decode model described in section~\ref{section:gnn_policy}. As the output is to be only for one edge, unlike the simpler \ac{gnn} policy, we read it not from the output edge attributes but the output global attributes. In this policy, the global attribute output is the 2-tuple defined in equation~\ref{equation:global_outputs} where the first element is the value to set on the edge specified and the second is the value of $\gamma$ to use for the softmin routing (although this is only read on the last iteration of a particular demand history).

\begin{equation}
  \label{equation:global_outputs}
  U = (\operatorname{weight}, \gamma)
\end{equation}

\subsection{Training}
As mentioned previously, for the iterative approach, it takes multiple actions from the agent to assign a single routing. This means we have to rethink how rewards are assigned. The previous approach of comparing to the optimal reward is still used. However, rather than giving rewards for partial routing assignments, we instead give a zero reward at each step and only give the real reward at the end of a set of actions when the routing for a demand history has been fully assigned. This method does work as long as the hyperparameters of the learning algorithm are tuned because it is especially dependent on the value of $\gamma$ used for discounting future rewards which allow for sensible policy updates to occur.

Another thing that we had to do was choosing an order to assign edge weights in the iterations. The simplest method would be using some sorted order. However, this could lead to the danger of the policy learning this fixed order and so overfitting and not being able to generalise. To avoid this issue, we always randomise the order that edges are assigned in training.

