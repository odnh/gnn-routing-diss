\chapter{Evaluation}
\label{chapter:evaluation}

\section{Aims}
So far we have described the problem, approaches to a solution, and how to build different \ac{rl} approaches that seek to solve it. The aim stated at the beginning of this work was to use \ac{rl} to perform generalisable data-driven routing. Therefore, this evaluation seeks to assess:

\begin{enumerate}
\item Are the different policies able to learn to provide a near-optimal routing for a demand?
\item Are the policies able to generalise this technique to unseen sequences of the same type?
\item Do they still generalise to unseen sequences from a different distribution?
\item Are they able to generalise onto different graphs using sequences form the same and different distributions?
\item Are these routing schemes applicable to real-world scenarios?
\end{enumerate}

In the course of this chapter these questions will be answered by a sequence of experiments testing all of these stated aims.


\section{Experimental setup}
The experiments were all performed (both training and testing models) using the codebase available in the git repository for this project\footnote{\url{https://github.com/odnh/cst-iii-project}} and following the instructions there are fully replicable by running only one script. Prior to running the experiments, the hyperparameters for the learning algorithm were tuned using a custom OpenTuner\cite{ansel2014opentuner} (which resides in the same repository) program to make sure that performance for all policies was at the highest levels we could achieve.

For the reinforcement learning we used the implementation of \ac{ppo} (PPO2) from the stable-baselines\cite{stable-baselines} library as the learning algorithm with its policy network replaced by the custom policies described in chapter~\ref{chapter:learning}. All training and evaluation was performed using this library. The \ac{gnn} policies themselves were implemented using the Graph Nets\cite{battaglia2018relational} library from Deepmind on top of TensorFlow\cite{tensorflow2015-whitepaper}. The entire environment was implemented in Python\cite{10.5555/1593511}, with a heavy reliance on features from NumPy\cite{oliphant2006guide} and NetworkX\cite{hagberg2008exploring} for numerical computation and working with graphs respectively.

The hardware used for all the experiments was a single virtual machine running Ubuntu Linux 18.04 with access to 8 CPU cores and an NVIDIA Quadro RTX 8000 GPU.


\subsection{Difficulties}
Combining \ac{rl} libraries with \ac{gnn} libraries was in fact harder than originally anticipated. This is because a \ac{gnn} can have a variable sized input and output whilst maintaining a fixed number of trainable variables. Most \ac{rl} algorithms require a fixed sized output to function to train correctly. The \ac{gnn} iterative approach does have a fixed output size (although the non-iterative approach does not). However, for simplicity most \ac{rl} libraries assume a fixed size input to the algorithm. Although \ac{ppo} is on-policy and the policy itself (a \ac{gnn}) can take a variable-sized input and return a fixed-size output most \ac{rl} libraries do not support this behaviour. Therefore, achieving interoperability between stable-baselines and Graph Nets required extensive work. This was mainly because Graph Nets maintains the ability to batch while having variable sizes by flattening batches into linear inputs whilst all \ac{rl} libraries use an extra tensor dimension which does not allow for variable sizes. Fortunately, a work-around was possible but it did have a performance impact for the training time.


\section{Baselines}
In order to be able to tell how good the results of the new methods are we require a comparison. For this comparison we decided to use two different types of baseline. The first of these is a simple \ac{mlp}-only policy as introduced by Valadarsky. This is because it is the method that we are seeking to improve upon by adding the ability to generalise over different graphs without impacting performance in predicting good routing strategies. We also add a simple \ac{lstm} strategy with the aim of showing that the fixed memory length imposed by how we have structured the policies does not overly weaken how the agent interprets and learns temporal regularity.

The second baseline is to show that the routings the policies learn and produce are in fact useful. In the chapter~\ref{chapter:background} we introduced the idea of an oblivious routing scheme, one which aims to provide a good routing strategy regardless of the particular demands on the network at any one time. As discussed, there is a lot of research in this area with many examples of promising results. However, in the absence of any readily-available open source code it was hard to combine any of these complex algorithms with the environment. Therefore, as an oblivious baseline we have used shortest-path routing as it is already the most common way that routing is performed within \acp{as}, is easy to implement without errors, and provides reasonable performance.

\section{Experiments}

\subsection{Learning static routing}
\label{section:exp_static}
The first experiment was designed to discover if, given a demand matrix, each of the policies would be able to learn to route the traffic over the graph with a low link utilisation. The reason for this experiment is that this ability is a prerequisite for being able to predict good routing strategies for the next step given a demand history as that is the same problem but with less exact information.

We chose to use the Abilene graph from the Topology Zoo dataset for these experiments and training was performed using 10 length one sequences. The policy history length was also one containing the same demand matrix. We then performed two different types of test. The first training and test was using a single demand matrix. The second used a set of different demand matrices for training and another set of different demand matrices for testing.

\begin{figure}
    \centering
    \input{figures/exp1.pgf}
    \caption{Learning to route given a \ac{dm}. Bar heights are the mean ratio between achieved max-link-utilisation and the optimal for the given DM. Dotted lines across the graph are the ratios achieved by the oblivious routing scheme.}
    \label{fig:exp_static}
\end{figure}

The results of this experiment can be seen in figure \ref{fig:exp_static}. From this we can see that all policies outperform the baseline which was expected as the baseline does not allow for multipath routing. We an also see that for all policies, tests over the same single \ac{dm} used in training perform better than when training and testing on multiple \acp{dm}. This is to be expected as we would assume that when just a single \ac{dm}, the policies could learn the most optimal routing that they can represent. Then, when we bring multiple \acp{dm} into the mix this means the internal structure cannot be solely focussed on optimising for one \ac{dm} so there will be some loss of performance. However, this does also present the issue that there is a problem with generalisation as if the policies could generalise perfectly we would expect to see no difference but this is unfortunately not the case. This issue is explored further in section~\ref{section:overfit}.


\subsection{Learning from temporal regularities}
As one express aim of this project was to predict good routing strategies to use given a history of traffic information, the next step was to verify if the policies are able to learn to give good routings for sequences which exhibit some form of temporal regularity. For this, we decided to train and test on cyclical and averaging sequences of bimodal demand matrices and gravity demand matrices with varying sparsity (as defined in section~\ref{section:demands}). This is because they are both are good examples of realistic traffic patterns and the cycles and averaging are strong forms of regularity so are a good test for showing if learning to predict routing strategies based on regularity will work.

Again, we used the Abilene graph for these experiments. The sequences used to train on were N different cyclic sequences with cycle length N, total sequence length N, and the memory length for the policy was 10. For testing, sequences of the same cycle lengths were used but with different demand matrices creating the cycles. Tests were performed for both the bimodal and gravity \acp{dm} and over both cyclical and averaging sequences.

\begin{figure}
    \centering
    \input{figures/exp2.pgf}
    \caption{Learning to route temporal regularities. Bar heights are the mean ratio between achieved max-link-utilisation and the optimal for the given \ac{dm}. Dotted lines across the graph are the ratios achieved by the oblivious routing scheme. The blue doted line is unfortunately underneath and obscured by the orange dotted line.}
    \label{fig:exp_cyclic}
\end{figure}

The results of this experiment can be seen in figure \ref{fig:exp_cyclic}. From this figure we can see that unfortunately all of the policies perform relatively badly. This is likely due to reasons discussed in detail in section~\ref{section:overfit}. Beyond the relative performance of different policies be can draw further conclusions from this plot. The first is that even though, as expected, different types of traffic distribution will cause different levels of link utilisation it is also the case that our softmin routing representation is able to achieve results closer to the optimum utilisation for some types of traffic than others. This can be seen from the fact that the ratio between the achieved utilisation and optimal utilisation is always less for the sequences built using bimodal \acp{dm} than those built using sparsified gravity \acp{dm} for all policies. On the other hand, the type of regularity induced (a cyclic or averaging sequence) does not seem to have the same kind of impact as it is not the case that all the policies perform better for one type of sequence than the other. However this may be due to reasons discussed in section~\ref{section:overfit}.

\subsection{Generalising to different cycle lengths}
The paper by Valadarsky et al. looked into learning regularity but only when using a fixed cycle length. We sought to examine whether the policies can in fact learn to generalise to different cycle lengths or not. To perform this we trained on cyclical bimodal sequences with different cycle lengths (2, 3, 4 and 5) and then tested on different sequences generated in the same way.

\begin{figure}
    \centering
    \input{figures/exp3.pgf}
    \caption{Generalising to different cycle lengths. Bar heights are the mean ratio between achieved max-link-utilisation and the optimal for the given \ac{dm}. Dotted lines across the graph are the ratios achieved by the oblivious routing scheme.}
    \label{fig:exp_vary}
\end{figure}

The results of this experiment can be seen in figure \ref{fig:exp_vary}. The first thing we can notice from this is that all of he policies outperform the baseline which is a good as it shows they are an improvement. Also interesting to note is that the mean ratio for all policies seems to be the same apart from the \ac{lstm} policy. This suggests that, as expected, the structure of history for the non-\ac{lstm} policies is too rigid and therefore structured in such a way that makes it hard for them to learn to deal with different lengths of cyclical regularity. The \ac{lstm} policy benefits from the fact that it has no specific history length assumed in its design meaning that it may be more flexible to interpreting different types of regularity which is what we seem to be seeing here. This suggests that \acp{lstm} would be interesting to combine with other techniques in further research.

\subsection{Generalising to unseen graphs}
As the other express aim of this work was to provide a policy that could generalise across different network graphs, it was crucial to test is this worked successfully. For this experiment we anticipate two different types of graph generalisation. The first is generalising to graphs that are the same but with slight changes, as we would still want the policy to work on a network where for example an outage has caused a link or node to temporarily drop. Furthermore, adding a new node or link should similarly not cause the policy to misbehave. The second type of generalisation is to graphs that are unrelated to the original graph, for example all drawn from different generating sets.

For this experiment we test both types of generalisation. For the first we train on the Abilene graph with a subset of vertices and edges randomly dropped and added and testing is performed using the same graph but a different set of small modifications. For the second type of generalisation we train on a set of graphs from the Topology Zoo dataset and test on a different set of graphs from the same dataset.

\begin{figure}
    \centering
    \input{figures/exp4.pgf}
    \caption{Generalising to unseen graphs. Bar heights are the mean ratio between achieved max-link-utilisation and the optimal for the given \ac{dm}. Dotted lines across the graph are the ratios achieved by the oblivious routing scheme.}
    \label{fig:exp_graphs}
\end{figure}

The results of this experiment can be seen in figure \ref{fig:exp_graphs}. We can see that in most cases the policies perform better than the baseline. In fact, the \ac{gnn} and iterative policies both perform better for small graph modifications, but only the iterative method performs better in the case of graphs that are very different from each other. The reason for this performance difference between the iterative and non-iterative methods is likely due to how the learning algorithm works. As the output size of the policies is fixed, this means that for smaller graphs for the \ac{gnn} policy some unused outputs had to be masked. However the learning algorithm does not know this and can still use those outputs when updating the policy which can lead to issues such as the lower performance we see here. The likely reason that the \ac{gnn} policy performs better for more similar graphs is that the above issue does not cause a large enough impact and the policy is simpler, therefore requiring less training and being more able to easily represent the complex relationships required.

\subsection{Real world}
Finally, as the problem we are proposing a solution to is a real-world problem, it is important to see if we get any meaningful performance on a real-world dataset. Fortunately there is a large dataset of real traffic matrices available from the Totem\cite{uhlig2006providing} project. Unfortunately the graph is much larger than in previous experiments and the time period that each measurement is valid for is fairly short meaning that the memory length for the policies had to be extended. However, we still proceeded with the experiment.

We trained on 5 sequences of length 20 with a memory length of 10 for the policies. Testing was performed in much the same way as for previous experiments, with the test sequences being selected randomly from the dataset but being different sequences to those being used for training.

\begin{figure}
    \centering
    \input{figures/exp5.pgf}
    \caption{Learning to route from TOTEM dataset. Bar heights are the mean ratio between achieved max-link-utilisation and the optimal for the given \ac{dm}. Dotted line across the graph is the ratio achieved by the oblivious routing scheme.}
    \label{fig:exp_real}
\end{figure}

The results of this experiment can be seen in figure \ref{fig:exp_real}. As with previous experiments we can see that the policies seem to outperform the baseline, however the \ac{mlp} and \ac{lstm} policies both have better performance than the \ac{gnn} policies. This is likely down to reasons described in the next section and the small size of the learning set (which was restricted as the TOTEM graph was significantly larger than other graphs tested on which greatly impacted computation time for calculating real and optimal link utilisation statistics) which result in a static oblivious routing being learned. It is probable that the oblivious routing learned by the two \ac{gnn} policies was not as effective or general as that learned by the other two policies. However, further analysis and experimentation would be required to prove if this is the case.


\section{Learning issues}
\label{section:overfit}
From the results we have seen that, most of the time, the \ac{rl} policies do seem to outperform the baseline. However, it should be noted that as the baseline is shortest-path routing, this shows that the policies are learning to reduce max-link-utilisation but not how well they are doing this task compared to best-in-class oblivious routing strategies. In fact, upon further investigation, it seems that the main issue is that given a high enough number of different sequences to train on, all the policies learn a fairly good oblivious routing.

To explore the point where overfitting particular \acp{dm} turns into providing a static oblivious routing scheme we ran a few more experiments. We began by revisiting the experiment discussed in section~\ref{section:exp_static}. We performed this experiment for the \ac{mlp} policy varying the number of \acp{dm} in the training set and testing on those same \acp{dm}.

In figure~\ref{fig:exp_fail} we can see both the overall performance of the trained models as the size of their training sets increases and the maximum difference between edge weights in different actions they output. From this we can gather two things: first that the performance gets worse the more different \acp{dm} we have to look at in training (for both training and when using the trained model) and second that the variance decreases until the point that whatever the input, the output remains effectively the same. The error area does widen for the training set plot, but this is simply an effect of adding more \acp{dm} which all have different relations to the optimal routing from each other.

\begin{figure}
    \centering
    \input{figures/overfit.pgf}
    \caption{Exploring when learning collapses to a single value. Right-hand axis corresponds to the green points and left-hand axis corresponds to the others. Blue plot is result of testing models on \acp{dm} from the training set whereas orange line is from \acp{dm} outside of the training set. Error area is standard deviation.}
    \label{fig:exp_fail}
\end{figure}


\section{Discussion}
We have seen various results in this chapter from experiments that sought to assess generalisability of the different policies. Overall, we have learnt that all the policies are able to outperform the baseline. We have also found that the policies struggle to fit the data well, in almost all cases overfitting as the set of demands to route grows in the training set. This is unfortunate as it suggests that these policies are in fact not structured in such a way that allows them to learn this information or that a different technique should be used to aid learning. However, even with this overfitting it does seem to be the case that the graph network-based policies are still able to generalise to other graphs well which is something we sought to show, even if what they are generalising is an oblivious routing as opposed to a strategy informed by demand histories.

In examining where overfitting occurs, it seems to be the case that either none of the policies are able to represent the relations required, or that the \ac{rl} algorithm used in tandem with the construction of training sets was not able to explore the trajectories available to a wide enough degree to find the best parameters for the models. When designing these experiments and the policies themselves we did try many variations in the policy network architectures for all policy types and, as mentioned previously, we performed hyperparameter tuning with the aim of improving results. However, even with these measures we were unfortunately unable to improve results further.

However, we have seen that the policies are able to fit small numbers of demand matrices and that graph neural networks are able to successfully generalise to different input graphs. This suggests that it is still a viable area for further research, especially in using \acp{gnn}, but that different techniques should be explored in relation to the learning.
