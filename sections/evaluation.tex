\chapter{Evaluation}
\label{chapter:evaluation}

\section{Aims}
So far, we have described the problem, approaches to a solution, and how to build different \ac{rl} approaches that seek to solve it. The aim stated at the beginning of this work was to use \ac{rl} to perform generalisable data-driven routing. Therefore, this evaluation seeks to assess:

\begin{enumerate}
\item Are the different policies able to learn to provide a near-optimal routing for a demand?
\item Are the policies able to generalise this technique to unseen sequences of the same type?
\item Do they still generalise to unseen sequences from a different distribution?
\item Are they able to generalise onto different graphs using sequences form the same and different distributions?
\item Are these routing schemes applicable to real-world scenarios?
\end{enumerate}

Over the course of this chapter, these questions will be answered by a sequence of experiments testing all of the stated aims.


\section{Experimental setup}
The experiments were all performed (both training and testing models) using the codebase available in the git repository for this project\footnote{\url{https://github.com/odnh/gnn-routing}}. Following the instructions there, the experiments are fully replicable by running only one script. Prior to running the experiments, the hyperparameters for the learning algorithm were tuned using a custom OpenTuner\cite{ansel2014opentuner} (which resides in the same repository) program to ensure that performance for all policies was at the highest levels we could achieve.

For the reinforcement learning, we used the implementation of \ac{ppo} (PPO2) from the stable-baselines\cite{stable-baselines} library as the learning algorithm with its policy network replaced by the custom policies described in chapter~\ref{chapter:learning}. All training and evaluation were performed using this library. The \ac{gnn} policies themselves were implemented using the Graph Nets\cite{battaglia2018relational} library from Deepmind on top of TensorFlow\cite{tensorflow2015-whitepaper}. The entire environment was implemented in Python\cite{10.5555/1593511}, with a heavy reliance on features from NumPy\cite{oliphant2006guide} and NetworkX\cite{hagberg2008exploring} for numerical computation and working with graphs, respectively.

The hardware used for all the experiments was a single virtual machine running Ubuntu Linux 18.04 with access to 8 CPU cores and an NVIDIA Quadro RTX 8000 GPU.

\subsection{Graph used}

For the majority of the experiments, we used the Abilene graph from the Topology Zoo dataset. We chose this graph because it is relatively small, which reduces learning time while still being large enough to exhibit enough complexity to ensure the results are interesting and useful. Separately, we ran smaller tests on other graphs from the dataset and achieved similar results. The graph itself can be seen in figure~\ref{fig:abilene}. It has a total of 11 vertices, 28 edges (or 14 bidirectional edges) and is representative of the structure of other \ac{as} graphs, which are generally planar and of a similar scale.

\begin{figure}
    \centering
    \resizebox{\textwidth}{!}{\input{figures/abilene.tex}}
    \caption{Abilene network graph topology.}
    \label{fig:abilene}
\end{figure}


\subsection{Difficulties}
Combining \ac{rl} libraries with \ac{gnn} libraries was more difficult than initially anticipated. This is because a \ac{gnn} can have a variable-sized input and output whilst maintaining a fixed number of trainable variables. Most \ac{rl} algorithms require a fixed-sized output to function to train correctly. The \ac{gnn} iterative approach does have a fixed output size (although the non-iterative approach does not). However, for simplicity, most \ac{rl} libraries assume a fixed-size input to the algorithm. Although \ac{ppo} is on-policy and the policy itself (a \ac{gnn}) can take a variable-sized input and return a fixed-size output, most \ac{rl} libraries do not support this behaviour. Therefore, achieving interoperability between stable-baselines and Graph Nets required extensive work. This was chiefly because Graph Nets maintains the ability to batch while having variable sizes by flattening batches into linear inputs, whilst all \ac{rl} libraries use an extra tensor dimension which does not allow for variable sizes. Fortunately, a work-around was possible, but it did have a performance impact on the training time.


\section{Baselines}
In order to be able to tell how good the results of the new methods are, we require a comparison. For this comparison, we decided to use two different types of baseline. The first of these is a simple \ac{mlp}-only policy, as introduced by Valadarsky. We chose this as it is the method that we are seeking to improve upon by adding the ability to generalise over different graphs without impacting performance in predicting good routing strategies. We also add a simple \ac{lstm} strategy to examine if the fixed memory length imposed by how we have structured the policies does not overly weaken how the agent interprets and learns from temporal regularity.

The second baseline is to show that the routings the policies learn and produce are in fact useful. In chapter~\ref{chapter:background}, we introduced the idea of an oblivious routing scheme, one which aims to provide a good routing strategy regardless of the particular demands on the network at any one time. As discussed, there is much research in this area with many examples of promising results. However, in the absence of any readily-available open-source code, it was hard to combine any of these complex algorithms with the environment. Therefore, as an oblivious baseline, we have used shortest-path routing as it is the most common method of routing within networks, is easy to implement without errors, and provides reasonable performance.

\section{Experiments}

\subsection{Learning static routing}
\label{section:exp_static}
The first experiment was designed to discover if, given a demand matrix, each of the policies would be able to learn to route the traffic over the graph with a low link utilisation. The reason for this experiment is that the ability to route for a single \ac{dm} is a prerequisite for predicting good routing strategies for the next step given a demand history, as that is the same problem but with less exact information.

We chose to use the Abilene graph from the Topology Zoo dataset for these experiments, and training was performed using 100 different \acp{dm}. The policy history length was one containing the same demand matrix as that being tested. We then performed two different types of test. The first training and test were using a single demand matrix. The second used a set of different demand matrices for training (all 100) and another set of different demand matrices for testing (a different 100). For each experiment, the tests were run 10 times.

\begin{figure}
    \centering
    \input{figures/exp1.pgf}
    \caption{Learning to route given a \ac{dm}. Bar heights are the mean ratio between achieved max-link-utilisation and the optimal for the given DM. Dotted lines across the graph are the ratios achieved by the oblivious routing scheme.}
    \label{fig:exp_static}
\end{figure}

The results of this experiment can be seen in figure \ref{fig:exp_static}. From this, we can see that all policies outperform the baseline, which was expected as the baseline does not allow for multipath routing. We can also see that for all policies, tests over the same single \ac{dm} used in training perform better than when training and testing on multiple \acp{dm}. This result is as expected because we would assume that when using just a single \ac{dm}, the policies can learn the most optimal routing that they can represent. Then, when we bring multiple \acp{dm} into the mix, this means the internal structure cannot be solely focussed on optimising for one \ac{dm}, so there will be some loss of performance. However, this does also present the issue that there is a problem with generalisation, as if the policies could generalise perfectly, we would expect to see no difference; this is unfortunately not the case. This issue is explored further in section~\ref{section:overfit}.


\subsection{Learning from temporal regularities}
As one express aim of this project was to predict good routing strategies to use given a history of traffic information, the next step was to verify if the policies can learn to give good routings for sequences which exhibit some form of temporal regularity. For this, we decided to train and test on cyclical and averaging sequences of bimodal demand matrices and gravity demand matrices with varying sparsity (as defined in section~\ref{section:demands}). These sequences were chosen because they are both are good examples of realistic traffic patterns and the cycles and averaging are strong forms of regularity so are a good test for exploring whether learning to predict routing strategies based on regularity will work.

Again, we used the Abilene graph for these experiments. The sequences used to train on were 100 different cyclic sequences with cycle length 5, total sequence length 10, and the memory length for the policy was 10. For testing, sequences of the same cycle lengths were used but with different demand matrices creating the cycles. Tests were performed for both the bimodal and gravity \acp{dm} and over both cyclical and averaging sequences. For each experiment, the tests were run 10 times.

\begin{figure}
    \centering
    \input{figures/exp2.pgf}
    \caption{Learning to route temporal regularities. Bar heights are the mean ratio between achieved max-link-utilisation and the optimal for the given \ac{dm}. Dotted lines across the graph are the ratios achieved by the oblivious routing scheme. The blue dotted line is hard to see: underneath and mostly obscured by the orange dotted line.}
    \label{fig:exp_cyclic}
\end{figure}

The results of this experiment can be seen in figure \ref{fig:exp_cyclic}. From this figure, we can see that unfortunately, all of the policies perform relatively badly. This is likely due to the reasons discussed in detail in section~\ref{section:overfit}. Beyond the relative performance of different policies we can draw further conclusions from this plot. The first is that even though, as expected, different types of traffic distribution will cause different levels of link utilisation, it is also the case that our softmin routing representation is able to achieve results closer to the optimum utilisation for some types of traffic than others. This is evident from the fact that the ratio between the achieved utilisation and optimal utilisation is always less for the sequences built using bimodal \acp{dm} than those built using sparsified gravity \acp{dm} for all policies. On the other hand, the type of regularity induced (a cyclic or averaging sequence) does not seem to have the same kind of impact, as it is not the case that all the policies perform better for one type of sequence than the other. However, this may be due to the reasons discussed in section~\ref{section:overfit}.

\subsection{Generalising to different cycle lengths}
The paper by Valadarsky et al. looked into learning regularity but only when using a fixed cycle length. We sought to examine whether the policies can, in fact, learn to generalise to different cycle lengths or not. To perform this, we trained on 100 cyclical bimodal sequences with different cycle lengths (2, 3, 4 and 5) and then tested on a different 100 sequences generated in the same way. For each experiment, the tests were run 10 times.

\begin{figure}
    \centering
    \input{figures/exp3.pgf}
    \caption{Generalising to different cycle lengths. Bar heights are the mean ratio between achieved max-link-utilisation and the optimal for the given \ac{dm}. Dotted lines across the graph are the ratios achieved by the oblivious routing scheme.}
    \label{fig:exp_vary}
\end{figure}

The results of this experiment can be seen in figure \ref{fig:exp_vary}. The first point we can notice is that all of the policies outperform the baseline, which is positive as it shows they are an improvement. Also interesting to note is that the mean ratio for all policies seems to be the same, apart from the \ac{lstm} policy. This suggests that, as expected, the structure of history for the non-\ac{lstm} policies is too rigid and therefore structured in such a way that makes it hard for them to learn to deal with different lengths of cyclical regularity. The \ac{lstm} policy benefits from the fact that it has no specific history length assumed in its design, meaning that it may be more flexible to interpreting different types of regularity (which is what we seem to be seeing here). This suggests that \acp{lstm} would be interesting to combine with other techniques in further research.

\subsection{Generalising to unseen graphs}
As the second express aim of this work is to provide a policy that could generalise across different network graphs, it is crucial to test if this worked successfully. For this experiment, we anticipate two different types of graph generalisation. The first is generalising to graphs that are the same but with minor changes, as we would still want the policy to work on a network where, for example, an outage has caused a link or node to drop temporarily. Furthermore, adding a new node or link should similarly not cause the policy to misbehave. The second type of generalisation is to graphs that are unrelated to the original graph, for example, all drawn from different generating sets.

For this experiment, we test both types of generalisation. For the first, we train on the Abilene graph with a subset of vertices and edges randomly dropped and added (5 different modifications), and testing is performed using the same graph but a different set of small modifications (5 different modifications again). For the second type of generalisation, we train on a set of 5 graphs from the Topology Zoo dataset and test on a different set of 5 graphs from the same dataset. The tests were each run 10 times.

\begin{figure}
    \centering
    \input{figures/exp4.pgf}
    \caption{Generalising to unseen graphs. Bar heights are the mean ratio between achieved max-link-utilisation and the optimal for the given \ac{dm}. Dotted lines across the graph are the ratios achieved by the oblivious routing scheme.}
    \label{fig:exp_graphs}
\end{figure}

The results of this experiment can be seen in figure \ref{fig:exp_graphs}. We can see that in most cases, the policies perform better than the baseline. In fact, the \ac{gnn} and iterative policies both perform better for small graph modifications, but only the iterative method performs better in the case of graphs that are very different from each other. The reason for this performance difference between the iterative and non-iterative methods is likely due to how the learning algorithm works. As the output size of the policies is fixed, this means that for smaller graphs for the \ac{gnn} policy, some unused outputs had to be masked. However, the learning algorithm does not know this and can still use those outputs when updating the policy, which can lead to issues such as the lower performance we see here. The likely reason that the \ac{gnn} policy performs better for more similar graphs is that the above issue does not cause a significant enough impact and the policy is simpler, therefore requiring less training and being more able to represent the complex relationships required easily.

\subsection{Real world}
Finally, as the problem we are attempting to solve is a real-world problem, it is essential to see if we can achieve any meaningful performance on a real-world dataset. Fortunately, there is a large dataset of real traffic matrices available from the Totem\cite{uhlig2006providing} project. Unfortunately, the graph is much larger than in previous experiments and the time period that each measurement is valid for is relatively short, meaning that the memory length for the policies had to be extended. However, we still proceeded with the experiment.

We trained on 5 sequences of length 20 with a memory length of 10 for the policies. Testing was performed in much the same way as for previous experiments, with the test sequences selected randomly from the dataset but being different sequences to those used for training. Each test was run 10 times.

\begin{figure}
    \centering
    \input{figures/exp5.pgf}
    \caption{Learning to route from TOTEM dataset. Bar heights are the mean ratio between achieved max-link-utilisation and the optimal for the given \ac{dm}. Dotted line across the graph is the ratio achieved by the oblivious routing scheme.}
    \label{fig:exp_real}
\end{figure}

The results of this experiment can be seen in figure \ref{fig:exp_real}. As with previous experiments, we can see that the policies seem to outperform the baseline. However, both the \ac{mlp} and \ac{lstm} policies have better performance than the \ac{gnn} policies. This better performance is likely down to reasons described in the next section and the small size of the learning set (which was restricted as the TOTEM graph was significantly larger than other graphs tested on, which greatly impacted computation time for calculating real and optimal link utilisation statistics) which result in a static oblivious routing being learned. It is probable that the oblivious routing learned by the two \ac{gnn} policies was not as effective or general as that learned by the other two policies. However, further analysis and experimentation would be required to prove if this is the case.


\section{Learning issues}
\label{section:overfit}
From the results, we have seen that the \ac{rl} policies generally do seem to outperform the baseline. However, it should be noted that as the baseline is shortest-path routing, this shows that the policies are learning to reduce max-link-utilisation but not how well they are doing this task compared to best-in-class oblivious routing strategies. In fact, upon further investigation, it seems that the main issue is that, given a high enough number of different sequences to train on, all the policies learn a fairly good oblivious routing rather than reacting to their inputs.

To explore the point where overfitting particular \acp{dm} turns into providing a static oblivious routing scheme, we undertook a few more experiments. We began by revisiting the experiment discussed in section~\ref{section:exp_static}. We performed this experiment for the \ac{mlp} policy varying, the number of \acp{dm} in the training set and testing on those same \acp{dm}.

In figure~\ref{fig:exp_fail}, we can see both the overall performance of the trained models as the size of their training sets increases and the maximum difference between edge weights in different actions they output. From this we can gather two things: first that the performance gets worse the more different \acp{dm} we have to look at in training (for both training and when using the trained model) and second, that the variance decreases until the point that whatever the input, the output remains effectively the same. The error area does widen for the training set plot, but this is simply an effect of adding more \acp{dm}, which all have different relations to the optimal routing from each other.

\begin{figure}
    \centering
    \input{figures/overfit.pgf}
    \caption{Exploring when learning collapses to a single value. Right-hand axis corresponds to the green points, and left-hand axis corresponds to the others. Blue plot is the result of testing models on \acp{dm} from the training set, whereas the orange line is from \acp{dm} outside of the training set. Error area is the standard deviation.}
    \label{fig:exp_fail}
\end{figure}

The reason this occurs is probably due to how the learning is trading off between two extremes. In effect, it could either learn to provide an optimal routing for a predicted \ac{dm} or just provide an optimal oblivious routing. The optimal routing for the \ac{dm} is preferable, but if it gets the \ac{dm} prediction wrong, then the result could be worse for link utilisation than the oblivious scheme which is guaranteed to behave well no matter the actual \ac{dm} in the next timestep. The overfitting we are seeing is likely where the policy cannot find a good middle ground, and therefore an oblivious strategy performs better than a bad prediction, so that is what the learning converges on.


\section{Discussion}
We have seen various results in this chapter from experiments that sought to assess generalisability of the different policies. Overall, we have learnt that all the policies are able to outperform the baseline. We have also found that the policies struggle to fit the data well, in almost all cases overfitting as the size of the set of demands to route grows in the training set. This is unfortunate as it suggests that these policies are not structured in such a way that allows them to learn this information or that a different technique should be used to aid learning. However, even with this overfitting, it does seem to be the case that the graph network-based policies are still able to generalise to other graphs well which is positive as it is something we sought to show; even if what they are generalising is an oblivious routing as opposed to a strategy informed by demand histories.

In examining where overfitting occurs, it seems to be the case that either none of the policies are able to represent the relations required, or that the \ac{rl} algorithm used in tandem with the construction of training sets was not able to explore the trajectories available to a sufficiently wide degree to find the best parameters for the models. When designing these experiments and the policies themselves, we tried many variations in the policy network architectures for all policy types and, as mentioned previously, we performed hyperparameter tuning with the aim of improving results. However, even with these measures, we were, unfortunately, unable to improve results further.

Overall, we have seen that the policies are able to fit small numbers of demand matrices and that graph neural networks are able to successfully generalise to different input graphs. This suggests that it is a viable area for further research, especially in using \acp{gnn}, but that different techniques should be explored in relation to the learning.

