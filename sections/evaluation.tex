\chapter{Evaluation}
\label{chapter:evaluation}

\subsection{Aims}
\todo[inline]{Give (and discuss) goals of what evaluation seeks to show (should map onto contributions given earlier). Talk about progressive randomisation and maybe include the definition table). summarise what experiments will be performed}
So far we have described the problem, approaches to a solution, and how to build different RL approaches that seek to solve it. The aim stated at the beginning of this work was to use RL to perform generalisable data-driven routing. Therefore, this evaluation seeks to assess:

\begin{enumerate}
\item Are the different policies able to learn to provide a near-optimal routing for a demand?
\item Are the policies able to generalise this technique to unseen sequences of the same type?
\item Do they still generalise to unseen sequences from a different distribution?
\item Are they able to generalise onto different graphs using sequences form the same and different distributions?
\item Are these routing schemes applicable to real-world scenarios?
\end{enumerate}

In the course of this chapter these questions will be answered by a sequence of experiments testing all of these stated aims.

\todo[inline]{talk about progresive randomisation here?}


\section{Experimental setup}
The experiments were all performed (both training and testing models) using the codebase available in the git repository for this project\footnote{\url{https://github.com/odnh/cst-iii-project}} and following the instructions there are fully replicable by running only one script. Prior to running the experiments, the hyperparameters for the learning algorithm were tuned using a custom OpenTuner\cite{ansel2014opentuner} (which resides in the same repository) to make sure that performance for all policies was at the highest levels we could achieve.

For the reinforcement learning we used the implementation of PPO (PPO2) from the stable-baselines\cite{stable-baselines} library as the learning algorithm with its policy network replaced by the custom policies described in chapter~\ref{chapter:learning}. All training and evaluation was performed using this library.

Further details on the precise set up used to obtain the results can be found in appendix~\ref{appendix}.
\todo[inline]{add deets about the machine and any seeds used to the correct appendix}

\subsection{Difficulties}
\todo[inline]{Talk about the massive issues combining graph\_nets and stable-baselines}

\section{Baselines}
In order to be able to tell how good the results of the new methods are we require a comparison. For this comparison we decided to use two different types of baseline. The first of these is a simple MLP-only policy as introduced by Valadarsky. This is because it is the method that we are seeking to improve upon by adding the ability to generalise over different graphs without impacting performance in predicting good routing strategies. We also add a simple LSTM strategy with the aim of showing that the fixed memory length imposed by how we have structured the policies does not overly weaken how the agent interprets and learns temporal regularity.

The second baseline is to show that the routings the policies learn and produce are in fact useful. In the chapter~\ref{chapter:background} we introduced the idea of an oblivious routing scheme, one which aims to provide a good routing strategy regardless of the particular demands on the network at any one time. As discussed, there is a lot of research in this area with many examples of promising results. However, in the absence of any readily-available open source code it was hard to combine any of these complex algorithms with the environment. Therefore, as an oblivious baseline we have used shortest-path routing as it is already the most common way that routing is performed within ASs, is easy to implement without errors, and provides reasonable performance.

\section{Experiments}

\subsection{Learning static routing}
\todo[inline]{talk about single sequence, single cycle}
The first experiment was designed to discover if, given a demand matrix, each of the policies would be able to learn to route the traffic over the graph with a low link utilisation. The reason for this experiment is that this ability is a prerequisite for being able to predict good routing strategies for the next step given a demand history as that is the same problem but with less exact information.

We chose to use the Abilene graph from the Topology Zoo dataset for these experiments and training was performed using N length one sequences. The policy history length was also one containing the same demand matrix. We then performed two different types of test. The first training and test was using a single demand matrix. THe second used a set of different demand matrices for training and another set of different demand matrices for testing.

\begin{figure}
    \centering
%    \input{figures/exp1.pgf}
    \caption{TODO: ADD PROPER CAPTION}
    \label{fig:exp_static}
\end{figure}

The results of this experiment can be seen in figure \ref{fig:exp_static}. NOW MUST DISCUSS RESULTS.


\subsection{Learning from temporal regularities}
As one express aim of this project was to predict good routing strategies to use given a history of traffic information, the next step was to verify if the policies are able to learn to give good routings for sequences which exhibit some form of temporal regularity. For this, we decided to train and test on cyclical and averaging sequnces of bimodal demand matrices and gravity demand matrices with varying sparsity (as defined in section~\ref{section:demands}). This is because they are both are good examples of realistic traffic patterns and the cycles and averaging are strong forms of regularity so are a good test for showing if learning to predict routing strategies based on regularity will work.

Again, we used the Abilene graph for these experiments. The sequences used to train on were N different cyclic sequences with cycle length N, total sequence length N, and memory length for the policy was 10. For testing, sequences of the same cycle lengths were used but with different demand matrices creating the cycles. Tests were performed for both the bimodal and gravity DMs and over both cyclical and averaging sequences.

\begin{figure}
    \centering
%    \input{figures/exp2.pgf}
    \caption{TODO: ADD PROPER CAPTION}
    \label{fig:exp_cyclic}
\end{figure}

The results of this experiment can be seen in figure \ref{fig:exp_cyclic}. NOW MUST DISCUSS RESULTS.

\subsection{Generalising to different sequence lengths}
The paper by Valadarsky et al. looked into learning regularity but only when using a fixed cycle length. We sought to examine whether the policies can in fact learn to generalise to different cycle lengths or not. To perform this we trained on cyclical bimodal sequences with different cycle lengths and then tested on different sequences generated in the same way.

\begin{figure}
    \centering
%    \input{figures/exp3.pgf}
    \caption{TODO: ADD PROPER CAPTION}
    \label{fig:exp_vary}
\end{figure}

The results of this experiment can be seen in figure \ref{fig:exp_vary}. NOW MUST DISCUSS RESULTS.

\subsection{Generalising to unseen graphs}
As the other express aim of this work was to provide a policy that could generalise across different network graphs, it was crucial to test is this worked successfully. For this experiment we anticipate two different types of graph generalisation. The first is generalising to graphs that are the same but with slight changes, as we would still want the policy to work on a network where for example an outage has caused a link or node to temporarily drop. Furthermore, adding a new node or link should similarly not cause the policy to misbehave. The second type of generalisation is to graphs that are unrelated to the original graph, for example all drawn from different generating sets.

For this experiment we test both types of generalisation. For the first we train on the Abilene graph with a subset of vertices and edges randomly dropped and added and testing is performed using the same graph but a different subset of modifications. For the second type of generalisation we train on a set of graphs from the Topology Zoo dataset and test on a different set of graphs from the same dataset.

\begin{figure}
    \centering
%    \input{figures/exp4.pgf}
    \caption{TODO: ADD PROPER CAPTION}
    \label{fig:exp_graphs}
\end{figure}

The results of this experiment can be seen in figure \ref{fig:exp_graphs}. NOW MUST DISCUSS RESULTS.

\subsection{Real world}
Finally, as the problem we are proposing a solution to is a real-world problem, it is important to see if we get any meaningful performance on a real-world dataset. Fortunately there is a large dataset of real traffic matrices available from the Totem\cite{uhlig2006providing} project. Unfortunately the graph is much larger than in previous experiments and the time period that each measurement is valid for is fairly short meaning that the memory length for the policies had to be extended. However, we still proceeded with the experiment.

We trained on N sequences of length N with a memory length of N for the policies. Testing was performed in much the same way as for previous experiments, with the test sequences being selected randomly from the dataset but being different sequences to those being used for training.

\begin{figure}
    \centering
%    \input{figures/exp5.pgf}
    \caption{TODO: ADD PROPER CAPTION}
    \label{fig:exp_real}
\end{figure}

The results of this experiment can be seen in figure \ref{fig:exp_real}. NOW MUST DISCUSS RESULTS.

\section{Discussion}
\todo[inline]{full, rounded discussion of results and weaknesses/issues with techniques}

